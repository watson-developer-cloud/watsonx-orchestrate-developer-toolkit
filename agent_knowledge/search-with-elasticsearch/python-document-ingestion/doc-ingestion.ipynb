{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a64f3d-1fae-4c63-bf90-bbc7b66b27f9",
   "metadata": {},
   "source": [
    "## Ingest PDF or Office documents using Tika, LangChain and Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894730b-fddf-4ad9-96d3-cb32a4f53de6",
   "metadata": {},
   "source": [
    "First, we will import all the necessary libraries and fetch variable values that we have previously setup using the previous defined environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f6e1d-f89a-4aa3-9f47-0be01f0e7729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import fnmatch\n",
    "import tqdm\n",
    "from tika import parser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from ssl import create_default_context\n",
    "from elasticsearch import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c3f18-484a-439e-86a0-4023769f206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_url = os.getenv('ES_URL')\n",
    "es_user = os.getenv('ES_USER')\n",
    "es_password = os.getenv('ES_PASSWORD')\n",
    "es_cacert = os.getenv('ES_CACERT')\n",
    "es_index_name = os.getenv('ES_INDEX_NAME')\n",
    "docs_dir = os.getenv('DOCS_DIR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7692cd-734c-46e6-bdeb-343f79e0ad0c",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf85840-7cb1-4185-8b19-43cd5d10d58f",
   "metadata": {},
   "source": [
    "Next, we define the utility functions below that we will use later on to initialize Elasticsearch and generate a dictionary of documents to ingest into an Elasticsearch index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2efda-ad13-41f1-805f-386519bd0013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_elasticsearch(es_cacert, es_url, es_user, es_password):\n",
    "    context = create_default_context(cafile=es_cacert)\n",
    "    client = Elasticsearch(\n",
    "        es_url,\n",
    "        basic_auth=(es_user, es_password),\n",
    "        ssl_context=context,\n",
    "        request_timeout=480\n",
    "    )\n",
    "    return client\n",
    "\n",
    "def generate_documents(docs_dir, es_index_name, text_splitter):\n",
    "    doc_count = 0\n",
    "    documents = []\n",
    "    for filename in os.listdir(docs_dir):\n",
    "        f = os.path.join(docs_dir, filename)\n",
    "        if os.path.isfile(f):\n",
    "            with open(f) as file:\n",
    "                print(\"Processing \" + f)\n",
    "                doc_count += 1\n",
    "                parsed_document = parser.from_file(f)\n",
    "                document_text = parsed_document['content']\n",
    "                chunks = text_splitter.split_text(document_text)\n",
    "                for chunk_id, chunk_text in enumerate(chunks):\n",
    "                    source_dict = dict()\n",
    "                    document_dict = {\"_index\": es_index_name}\n",
    "                    source_dict['doc_id'] = doc_count\n",
    "                    source_dict['chunk_id'] = chunk_id\n",
    "                    source_dict['title'] = str(filename)\n",
    "                    source_dict['text'] = chunk_text\n",
    "                    document_dict['_id'] = hashlib.md5(chunk_text.encode('utf-8')).hexdigest()\n",
    "                    document_dict['_source'] = source_dict\n",
    "                    documents.append(document_dict)\n",
    "    return documents\n",
    "\n",
    "def yield_docs(documents):\n",
    "    for doc in documents:\n",
    "        yield doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c9b8e5-8620-4e1d-b679-892e4932f45b",
   "metadata": {},
   "source": [
    "## Using LangChain text splitting for chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af516b4-e181-47d3-b9d2-bbc4f235c7fd",
   "metadata": {},
   "source": [
    "We use the LangChain library to split our document text into smaller chunks. This is needed so as to fit text within our model's context window. For example, ELSER in Elasticsearch has a 512 token limit, so we need to ensure every chunk remains within that size so we can obtain embedded vectors on each of the 512 token chunks of the document text. \n",
    "\n",
    "You can update the code below and use different values of \"chunk_size\" and \"chunk_overlap\" depending on the downstream model that you are working with or try out other text splitters as well. You can read more about it in the [LangChain documentation](https://python.langchain.com/docs/modules/data_connection/document_transformers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd94fa98-4bd0-42d0-98bb-95b6177be422",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter =RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=124,\n",
    "    is_separator_regex=False,\n",
    "    disallowed_special=()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec98b1-1008-4c52-8d4b-c228caffad2e",
   "metadata": {},
   "source": [
    "## Index into Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec850f62-4235-425e-9fc3-f26754a0f851",
   "metadata": {},
   "source": [
    "We will now use the helpers to initialize an Elasticsearch client and ingest documents into an index upon breaking them into chunks as defined by our chunking process above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024d2c0-a35e-4114-b50e-2266ab15a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = init_elasticsearch(es_cacert, es_url, es_user, es_password)\n",
    "documents = generate_documents(docs_dir, es_index_name, text_splitter)\n",
    "number_of_docs = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d94524-69e1-4a93-9fe7-877a1845fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indexing documents...\")\n",
    "\n",
    "progress = tqdm.tqdm(unit=\"docs\", total=number_of_docs)\n",
    "successes = 0\n",
    "\n",
    "try:\n",
    "    for ok, action in helpers.streaming_bulk(\n",
    "    client=client,\n",
    "    chunk_size=50,\n",
    "    actions=yield_docs(documents),\n",
    "    ):\n",
    "        progress.update(50)\n",
    "        successes += ok\n",
    "except Exception as e:\n",
    "    print(e.errors)\n",
    "\n",
    "print(\"Indexed %d/%d documents\" % (successes, number_of_docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
